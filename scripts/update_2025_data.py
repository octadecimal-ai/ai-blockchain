#!/usr/bin/env python3
"""
Update 2025 Data
================
Skrypt do uzupe≈Çnienia danych BTC/USDC z 2025 roku do aktualnej daty.
Pobiera brakujƒÖce dane z Binance i aktualizuje plik CSV.
"""

import os
import sys
from pathlib import Path
from datetime import datetime, timezone
import pandas as pd
import json
from dotenv import load_dotenv

# Dodaj ≈õcie≈ºkƒô projektu
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Za≈Çaduj .env je≈õli istnieje
env_path = Path(__file__).parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)

from loguru import logger
from src.collectors.exchange.binance_collector import BinanceCollector

# Konfiguracja logowania
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
    level="INFO",
    colorize=True
)


def load_existing_data(csv_file: Path = None) -> pd.DataFrame:
    """
    Wczytuje istniejƒÖce dane z bazy danych (BTC/USDC) lub z CSV (fallback).
    
    Je≈õli csv_file jest None, pobiera dane z bazy danych.
    Je≈õli csv_file jest podany, u≈ºywa go jako fallback.
    """
    # Pr√≥buj najpierw z bazy danych
    if csv_file is None:
        try:
            from src.database.btcusdc_loader import load_btcusdc_from_db
            logger.info("üìÇ Wczytujƒô dane BTC/USDC z bazy danych...")
            df = load_btcusdc_from_db()
            
            if not df.empty:
                df = df.sort_index()
                logger.success(f"‚úÖ Wczytano {len(df)} ≈õwiec z bazy danych")
                logger.info(f"   Okres: {df.index[0]} ‚Üí {df.index[-1]}")
                return df
        except Exception as e:
            logger.warning(f"Nie uda≈Ço siƒô wczytaƒá z bazy danych: {e}, pr√≥bujƒô CSV...")
    
    # Fallback do CSV
    if csv_file is None:
        logger.error("‚ùå Brak pliku CSV i nie mo≈ºna wczytaƒá z bazy danych")
        return pd.DataFrame()
    
    if not csv_file.exists():
        logger.error(f"Plik nie istnieje: {csv_file}")
        return pd.DataFrame()
    
    logger.info(f"üìÇ Wczytujƒô istniejƒÖce dane z CSV: {csv_file}")
    df = pd.read_csv(csv_file, index_col=0, parse_dates=True)
    df = df.sort_index()
    
    logger.success(f"‚úÖ Wczytano {len(df)} ≈õwiec z CSV")
    logger.info(f"   Okres: {df.index[0]} ‚Üí {df.index[-1]}")
    
    return df


def fetch_missing_data(
    collector: BinanceCollector,
    start_date: datetime,
    end_date: datetime,
    symbol: str = "BTC/USDC",
    timeframe: str = "1h"
) -> pd.DataFrame:
    """Pobiera brakujƒÖce dane z Binance."""
    logger.info(f"üì• Pobieram brakujƒÖce dane: {start_date} ‚Üí {end_date}")
    
    df = collector.fetch_historical(
        symbol=symbol,
        timeframe=timeframe,
        start_date=start_date,
        end_date=end_date
    )
    
    if df.empty:
        logger.warning("‚ö†Ô∏è  Nie pobrano ≈ºadnych danych")
        return pd.DataFrame()
    
    logger.success(f"‚úÖ Pobrano {len(df)} nowych ≈õwiec")
    return df


def merge_data(existing_df: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    """≈ÅƒÖczy istniejƒÖce i nowe dane, usuwa duplikaty."""
    if existing_df.empty:
        return new_df
    
    if new_df.empty:
        return existing_df
    
    # Po≈ÇƒÖcz dane
    combined = pd.concat([existing_df, new_df])
    
    # Usu≈Ñ duplikaty (zachowaj pierwszy)
    combined = combined[~combined.index.duplicated(keep='first')]
    
    # Sortuj po dacie
    combined = combined.sort_index()
    
    logger.info(f"üìä Po≈ÇƒÖczono dane: {len(existing_df)} + {len(new_df)} = {len(combined)} (po usuniƒôciu duplikat√≥w)")
    
    return combined


def save_data(df: pd.DataFrame, csv_file: Path, metadata_file: Path, year: str = "2025"):
    """Zapisuje dane do CSV i aktualizuje metadata."""
    # Zapisuj CSV
    logger.info(f"üíæ Zapisujƒô dane do: {csv_file}")
    df.to_csv(csv_file)
    logger.success(f"‚úÖ Zapisano {len(df)} ≈õwiec do CSV")
    
    # Aktualizuj metadata
    first_price = float(df['close'].iloc[0])
    last_price = float(df['close'].iloc[-1])
    high_price = float(df['high'].max())
    low_price = float(df['low'].min())
    change_percent = ((last_price - first_price) / first_price) * 100
    
    # Oblicz volatility (odchylenie standardowe zmian cen)
    price_changes = df['close'].pct_change() * 100
    volatility_percent = float(price_changes.std())
    
    metadata = {
        "year": year,
        "symbol": "BTC/USDC",
        "timeframe": "1h",
        "start_date": df.index[0].strftime('%Y-%m-%dT%H:%M:%S'),
        "end_date": df.index[-1].strftime('%Y-%m-%dT%H:%M:%S'),
        "candles": len(df),
        "first_price": first_price,
        "last_price": last_price,
        "high_price": high_price,
        "low_price": low_price,
        "change_percent": change_percent,
        "volatility_percent": volatility_percent,
        "source": "Binance API",
        "data_file": csv_file.name
    }
    
    logger.info(f"üíæ Aktualizujƒô metadata: {metadata_file}")
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    logger.success("‚úÖ Metadata zaktualizowane")
    
    # Wy≈õwietl podsumowanie
    print("\n" + "=" * 70)
    print("üìä PODSUMOWANIE ZAKTUALIZOWANYCH DANYCH")
    print("=" * 70)
    print(f"Rok: {year}")
    print(f"Symbol: {metadata['symbol']}")
    print(f"Timeframe: {metadata['timeframe']}")
    print(f"≈öwiece: {metadata['candles']}")
    print(f"Okres: {metadata['start_date']} ‚Üí {metadata['end_date']}")
    print(f"Cena poczƒÖtkowa: ${metadata['first_price']:,.2f}")
    print(f"Cena ko≈Ñcowa: ${metadata['last_price']:,.2f}")
    print(f"Zmiana: {metadata['change_percent']:+.2f}%")
    print(f"Volatility: {metadata['volatility_percent']:.2f}%")
    print("=" * 70)


def main():
    """G≈Ç√≥wna funkcja."""
    # ≈öcie≈ºki plik√≥w
    data_dir = Path("data/backtest_periods/binance")
    csv_file = data_dir / "BTCUSDC_2025_1h.csv"
    metadata_file = data_dir / "BTCUSDC_2025_1h_metadata.json"
    
    # Sprawd≈∫ czy pliki istniejƒÖ
    if not csv_file.exists():
        logger.error(f"Plik CSV nie istnieje: {csv_file}")
        logger.info("Uruchom najpierw skrypt do pobrania danych z 2025 roku")
        sys.exit(1)
    
    # Wczytaj istniejƒÖce dane
    existing_df = load_existing_data(csv_file)
    
    if existing_df.empty:
        logger.error("Nie uda≈Ço siƒô wczytaƒá istniejƒÖcych danych")
        sys.exit(1)
    
    # Okre≈õl datƒô poczƒÖtkowƒÖ dla nowych danych (ostatnia ≈õwieca + 1h)
    last_timestamp = existing_df.index[-1]
    if isinstance(last_timestamp, pd.Timestamp):
        start_date = last_timestamp.to_pydatetime()
    else:
        start_date = pd.to_datetime(last_timestamp).to_pydatetime()
    
    # Dodaj 1 godzinƒô (nastƒôpna ≈õwieca)
    from datetime import timedelta
    start_date = start_date + timedelta(hours=1)
    
    # Data ko≈Ñcowa: dzisiaj 23:59:59
    end_date = datetime.now(timezone.utc).replace(hour=23, minute=59, second=59, microsecond=0)
    
    logger.info(f"üìÖ Okres do uzupe≈Çnienia: {start_date} ‚Üí {end_date}")
    
    # Sprawd≈∫ czy sƒÖ dane do pobrania
    if start_date >= end_date:
        logger.info("‚úÖ Wszystkie dane sƒÖ ju≈º aktualne!")
        logger.info(f"   Ostatnia ≈õwieca: {last_timestamp}")
        return
    
    # Pobierz brakujƒÖce dane
    collector = BinanceCollector(sandbox=False)
    new_df = fetch_missing_data(
        collector=collector,
        start_date=start_date,
        end_date=end_date,
        symbol="BTC/USDC",
        timeframe="1h"
    )
    
    if new_df.empty:
        logger.warning("‚ö†Ô∏è  Nie pobrano nowych danych - mo≈ºliwe ≈ºe dane sƒÖ ju≈º aktualne")
        return
    
    # Po≈ÇƒÖcz dane
    combined_df = merge_data(existing_df, new_df)
    
    # Zapisz zaktualizowane dane
    save_data(combined_df, csv_file, metadata_file, year="2025")
    
    logger.success("\nüéâ Dane zaktualizowane pomy≈õlnie!")


if __name__ == "__main__":
    main()

