#!/usr/bin/env python3
"""
LLM Sentiment Collector Daemon
===============================
Skrypt dzia≈ÇajƒÖcy w tle, kt√≥ry zbiera dane sentymentu z Reddit/Twitter (g≈Ç√≥wne ≈∫r√≥d≈Ça)
i analizuje je u≈ºywajƒÖc LLM. GDELT u≈ºywany tylko jako fallback.

U≈ºycie:
    python scripts/llm_sentiment_daemon.py
    python scripts/llm_sentiment_daemon.py --interval=600 --symbols=BTC/USDC

Autor: AI Assistant
Data: 2025-12-18
"""

import os
import sys
import time
import signal
import argparse
import json
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import List, Dict, Optional
import traceback

# Dodaj ≈õcie≈ºkƒô projektu
sys.path.insert(0, str(Path(__file__).parent.parent))

# Za≈Çaduj zmienne ≈õrodowiskowe z .env je≈õli istnieje
env_file = Path(__file__).parent.parent / '.env'
if env_file.exists():
    with open(env_file) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                # Usu≈Ñ cudzys≈Çowy je≈õli sƒÖ
                value = value.strip('"').strip("'")
                os.environ.setdefault(key, value)

from loguru import logger
from src.database.manager import DatabaseManager

# Spr√≥buj zaimportowaƒá web search engine
try:
    from src.utils.web_search import WebSearchEngine
    WEB_SEARCH_AVAILABLE = True
except ImportError:
    WEB_SEARCH_AVAILABLE = False
    logger.warning("WebSearchEngine niedostƒôpny")

# Spr√≥buj zaimportowaƒá Tavily Query Manager
try:
    from prompts.tavily_queries.query_manager import (
        TavilyQueryManager,
        QueryRotator
    )
    TAVILY_QUERY_MANAGER_AVAILABLE = True
except ImportError:
    TAVILY_QUERY_MANAGER_AVAILABLE = False
    logger.warning("TavilyQueryManager niedostƒôpny - u≈ºywam generycznych zapyta≈Ñ")

# Spr√≥buj zaimportowaƒá inne kolektory (jako fallback)
# GDELT ma sw√≥j osobny daemon (gdelt_sentiment_daemon.py) - nie u≈ºywamy go tutaj

try:
    from src.collectors.sentiment import TwitterCollector
    TWITTER_AVAILABLE = True
except ImportError:
    TWITTER_AVAILABLE = False

try:
    from src.collectors.sentiment import RedditCollector
    REDDIT_AVAILABLE = True
except ImportError:
    REDDIT_AVAILABLE = False

# Spr√≥buj zaimportowaƒá LLM analyzer
try:
    from src.collectors.sentiment import LLMSentimentAnalyzer
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    logger.error("LLMSentimentAnalyzer niedostƒôpny - zainstaluj: pip install anthropic")

# Mapowanie kraj√≥w na jƒôzyki (u≈ºywane tylko jako fallback je≈õli query_manager niedostƒôpny)
# UWAGA: Je≈õli query_manager jest dostƒôpny, u≈ºywa on REGION_TO_LANGUAGE z query_manager.py
# kt√≥re ma pe≈Çniejsze mapowanie (np. SG -> sg, nie en)
COUNTRY_LANGUAGES = {
    "US": "en",
    "GB": "en",
    "CN": "zh",
    "JP": "ja",
    "KR": "ko",
    "DE": "de",
    "RU": "ru",
    "SG": "sg",  # Poprawione: SG u≈ºywa sg.txt, nie en.txt
    "AU": "en",
    "FR": "fr",
    "ES": "es",
    "IT": "it",
    "NL": "nl",
    "CA": "en",
    "BR": "pt",
    "IN": "en",
    "HK": "zh",
    "CH": "de",
    "AE": "ar",
}

# Mapowanie kraj√≥w na regiony DuckDuckGo (format: 'region-language')
# U≈ºywane do wymuszenia lokalizacji i jƒôzyka wynik√≥w wyszukiwania
COUNTRY_TO_DUCKDUCKGO_REGION = {
    "US": "us-en",
    "GB": "uk-en",  # Wielka Brytania
    "CN": "cn-zh",
    "JP": "jp-jp",
    "KR": "kr-ko",
    "DE": "de-de",
    "RU": "ru-ru",
    "SG": "sg-en",  # Singapur - angielski
    "AU": "au-en",
    "FR": "fr-fr",
    "ES": "es-es",
    "IT": "it-it",
    "NL": "nl-nl",
    "CA": "ca-en",
    "BR": "br-pt",
    "IN": "in-en",
    "HK": "hk-zh",
    "CH": "ch-de",  # Szwajcaria - niemiecki
    "AE": "ae-ar",
    "PL": "pl-pl",
}

# Mapowanie kraj√≥w na jƒôzyki dla HTML scraping (format: 'language-COUNTRY')
# U≈ºywane jako parametr 'kl' w DuckDuckGo HTML scraping
COUNTRY_TO_DUCKDUCKGO_LANGUAGE = {
    "US": "en-US",
    "GB": "en-GB",
    "CN": "zh-CN",
    "JP": "ja-JP",
    "KR": "ko-KR",
    "DE": "de-DE",
    "RU": "ru-RU",
    "SG": "en-SG",
    "AU": "en-AU",
    "FR": "fr-FR",
    "ES": "es-ES",
    "IT": "it-IT",
    "NL": "nl-NL",
    "CA": "en-CA",
    "BR": "pt-BR",
    "IN": "en-IN",
    "HK": "zh-HK",
    "CH": "de-CH",
    "AE": "ar-AE",
    "PL": "pl-PL",
    "PL": "pl",
}

# Mapowanie kod√≥w kraj√≥w na nazwy (dla web search)
COUNTRY_NAMES = {
    "US": "United States",
    "GB": "United Kingdom",
    "CN": "China",
    "JP": "Japan",
    "KR": "South Korea",
    "DE": "Germany",
    "RU": "Russia",
    "SG": "Singapore",
    "AU": "Australia",
    "FR": "France",
    "ES": "Spain",
    "IT": "Italy",
    "NL": "Netherlands",
    "CA": "Canada",
    "BR": "Brazil",
    "IN": "India",
    "HK": "Hong Kong",
    "CH": "Switzerland",
    "AE": "United Arab Emirates",
    "PL": "Poland",
}


class LLMSentimentDaemon:
    """
    Daemon do zbierania i analizy sentymentu u≈ºywajƒÖc LLM.
    """
    
    def __init__(
        self,
        symbols: List[str] = None,
        countries: List[str] = None,
        query: str = "bitcoin OR BTC OR cryptocurrency",
        update_interval: int = 600,  # 10 minut w sekundach (zalecane: 600-1800 dla wystarczajƒÖcej ilo≈õci danych)
        database_url: Optional[str] = None,
        llm_model: str = "claude-3-5-haiku-20241022"  # Claude Haiku - ta≈Ñszy model ($0.25 vs $3.00/MTok)
    ):
        """
        Inicjalizuje daemon.
        
        Args:
            symbols: Lista symboli do analizy (domy≈õlnie: BTC/USDC)
            countries: Lista kraj√≥w do analizy (domy≈õlnie: top crypto markets)
            query: Zapytanie do GDELT
            update_interval: Interwa≈Ç aktualizacji w sekundach (domy≈õlnie: 600 = 10 min)
            database_url: URL bazy danych (domy≈õlnie: z .env lub SQLite)
            llm_model: Model LLM do u≈ºycia
        """
        self.symbols = symbols or ["BTC/USDC"]
        self.countries = countries or ["US", "CN", "JP", "KR", "DE", "GB", "RU", "SG"]
        self.query = query
        self.update_interval = update_interval
        self.running = False
        self.llm_model = llm_model
        
        # Inicjalizuj bazƒô danych
        if database_url is None:
            database_url = os.getenv('DATABASE_URL')
        
        self.db = DatabaseManager(database_url=database_url)
        self.db.create_tables()
        logger.info(f"Po≈ÇƒÖczono z bazƒÖ: {self.db._safe_url()}")
        
        # Inicjalizuj web search engine (g≈Ç√≥wne ≈∫r√≥d≈Ço)
        self.web_search = None
        if WEB_SEARCH_AVAILABLE:
            try:
                # DuckDuckGo jako domy≈õlny provider (z fallback do Tavily/Google)
                # Fallback: najpierw Tavily, potem Google (je≈õli DuckDuckGo nie powiedzie siƒô)
                provider = os.getenv('WEB_SEARCH_PROVIDER', 'duckduckgo')  # Domy≈õlnie DuckDuckGo
                
                self.web_search = WebSearchEngine(provider=provider)
                # DuckDuckGo nie wymaga API key, wiƒôc sprawdzamy tylko czy web_search zosta≈Ç utworzony
                if self.web_search:
                    if provider == 'duckduckgo':
                        logger.info(f"WebSearchEngine zainicjalizowany (provider: {provider} - darmowe, bez API key)")
                    elif self.web_search.api_key:
                        logger.info(f"WebSearchEngine zainicjalizowany (provider: {provider})")
                    else:
                        logger.warning(f"WebSearchEngine niedostƒôpny - brak API key dla {provider}")
                        self.web_search = None
                else:
                    logger.warning(f"WebSearchEngine nie zosta≈Ç utworzony dla {provider}")
            except Exception as e:
                logger.warning(f"WebSearchEngine niedostƒôpny: {e}")
                self.web_search = None
        
        # Inicjalizuj Tavily Query Manager dla spersonalizowanych zapyta≈Ñ regionalnych
        self.query_manager = None
        self.query_rotator = None
        if TAVILY_QUERY_MANAGER_AVAILABLE:
            try:
                # ≈öcie≈ºka do katalogu z zapytaniami (wzglƒôdem root projektu)
                queries_dir = Path(__file__).parent.parent / "prompts" / "tavily_queries"
                self.query_manager = TavilyQueryManager(str(queries_dir))
                self.query_rotator = QueryRotator(self.query_manager, reset_after_hours=24)
                logger.info(f"TavilyQueryManager zainicjalizowany: {queries_dir}")
            except Exception as e:
                logger.warning(f"TavilyQueryManager niedostƒôpny: {e}")
                self.query_manager = None
                self.query_rotator = None
        
        # Inicjalizuj inne kolektory jako fallback (je≈õli web search niedostƒôpny)
        # GDELT ma sw√≥j osobny daemon (gdelt_sentiment_daemon.py)
        # Nie u≈ºywamy GDELT w tym daemonie
        
        self.twitter_collector = None
        if TWITTER_AVAILABLE:
            try:
                self.twitter_collector = TwitterCollector()
                logger.info("TwitterCollector zainicjalizowany (fallback)")
            except Exception as e:
                logger.warning(f"TwitterCollector niedostƒôpny: {e}")
        
        self.reddit_collector = None
        if REDDIT_AVAILABLE:
            try:
                self.reddit_collector = RedditCollector()
                logger.info("RedditCollector zainicjalizowany (fallback)")
            except Exception as e:
                logger.warning(f"RedditCollector niedostƒôpny: {e}")
        
        # Inicjalizuj LLM analyzer
        if not LLM_AVAILABLE:
            raise ImportError("LLMSentimentAnalyzer niedostƒôpny - zainstaluj: pip install anthropic")
        
        try:
            self.llm_analyzer = LLMSentimentAnalyzer(
                model=self.llm_model,
                database_url=database_url,
                save_to_db=True
            )
            logger.info(f"LLMSentimentAnalyzer zainicjalizowany: {self.llm_model}")
        except Exception as e:
            logger.error(f"Nie mo≈ºna zainicjalizowaƒá LLMSentimentAnalyzer: {e}")
            raise
        
        # Statystyki
        self.stats = {
            "cycles_count": 0,
            "analyses_count": 0,
            "errors_count": 0,
            "total_cost_pln": 0.0,
            "last_update": None
        }
        
        # Obs≈Çuga sygna≈Ç√≥w
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Obs≈Çuguje sygna≈Çy zatrzymania."""
        logger.info(f"Otrzymano sygna≈Ç {signum} - zatrzymywanie...")
        self.running = False
    
    def _update_web_search_data_in_db(
        self,
        symbol: str,
        region: str,
        timestamp: datetime,
        web_search_query: str,
        web_search_response: str,
        web_search_answer: Optional[str],
        web_search_results_count: int
    ):
        """
        Aktualizuje rekord w bazie danych o dane Web Search.
        
        Args:
            symbol: Symbol kryptowaluty
            region: Kod regionu
            timestamp: Timestamp analizy
            web_search_query: Zapytanie do web search
            web_search_response: Pe≈Çna odpowied≈∫ z web search (JSON)
            web_search_answer: Podsumowanie AI z web search
            web_search_results_count: Liczba wynik√≥w
        """
        try:
            from src.database.models import LLMSentimentAnalysis
            from sqlalchemy import and_
            
            with self.db.get_session() as session:
                # Znajd≈∫ ostatni rekord dla danego symbolu i regionu z tym timestampem
                # (lub najbli≈ºszym, je≈õli dok≈Çadny timestamp nie istnieje)
                record = session.query(LLMSentimentAnalysis).filter(
                    and_(
                        LLMSentimentAnalysis.symbol == symbol,
                        LLMSentimentAnalysis.region == region,
                        LLMSentimentAnalysis.timestamp >= timestamp - timedelta(seconds=30),  # 30 sekund tolerancji
                        LLMSentimentAnalysis.timestamp <= timestamp + timedelta(seconds=30)
                    )
                ).order_by(LLMSentimentAnalysis.timestamp.desc()).first()
                
                if record:
                    # Zaktualizuj dane Web Search
                    record.web_search_query = web_search_query
                    record.web_search_response = web_search_response
                    record.web_search_answer = web_search_answer
                    record.web_search_results_count = web_search_results_count
                    session.commit()
                    logger.debug(f"Zaktualizowano dane Web Search dla {symbol} @ {region} @ {timestamp}")
                else:
                    logger.warning(f"Nie znaleziono rekordu do aktualizacji danych Web Search: {symbol} @ {region} @ {timestamp}")
        except Exception as e:
            logger.error(f"B≈ÇƒÖd aktualizacji danych Web Search w bazie: {e}")
            raise
    
    def _execute_web_search(self, search_query: str, country: str) -> Optional[tuple]:
        """
        Wykonuje pojedyncze zapytanie Web Search (fallback gdy QueryManager niedostƒôpny).
        
        Args:
            search_query: Zapytanie wyszukiwania
            country: Kod kraju (dla logowania)
        
        Returns:
            Tuple (texts, web_search_query, web_search_response, web_search_answer, web_search_results_count) lub None
        """
        try:
            logger.info(f"üîç Wyszukujƒô w internecie: {search_query}")
            
            # Pobierz region i jƒôzyk dla DuckDuckGo na podstawie kraju
            region = COUNTRY_TO_DUCKDUCKGO_REGION.get(country)
            language = COUNTRY_TO_DUCKDUCKGO_LANGUAGE.get(country)
            
            if region:
                logger.debug(f"   U≈ºywam region DuckDuckGo: {region}, jƒôzyk: {language}")
            
            search_results = self.web_search.search(
                query=search_query,
                max_results=5,
                search_depth="basic",
                include_answer=False,
                region=region,  # Region DuckDuckGo (np. 'us-en', 'de-de')
                language=language  # Jƒôzyk dla HTML scraping (np. 'en-US', 'de-DE')
            )
            
            texts = []
            if search_results.get("success") and search_results.get("results"):
                for result in search_results["results"]:
                    if "title" in result:
                        texts.append(result["title"])
                    if "content" in result:
                        texts.append(result["content"])
                    elif "snippet" in result:
                        texts.append(result["snippet"])
                
                logger.info(f"üåê Web Search: {len(texts)} tekst√≥w z {len(search_results['results'])} wynik√≥w")
                
                web_search_query = search_query
                # Zapisz pe≈ÇnƒÖ odpowied≈∫ z DuckDuckGo (zawiera results, query, timestamp, etc.)
                web_search_response = json.dumps(search_results, ensure_ascii=False)
                # DuckDuckGo nie zwraca "answer" jak Tavily - zawsze None
                web_search_answer = None
                web_search_results_count = len(search_results.get("results", []))
                
                return (texts, web_search_query, web_search_response, web_search_answer, web_search_results_count)
            else:
                error_msg = search_results.get("error", "Nieznany b≈ÇƒÖd")
                if "usage limit" in error_msg.lower() or "432" in str(error_msg):
                    logger.warning(f"‚ö†Ô∏è  Tavily: Przekroczono limit planu dla {country}")
                else:
                        logger.warning(f"‚ö†Ô∏è  Web Search nie zwr√≥ci≈Ç wynik√≥w: {error_msg}")
                return None
                
        except Exception as e:
            error_str = str(e)
            if "432" in error_str or "usage limit" in error_str.lower():
                logger.warning(f"‚ö†Ô∏è  Tavily: Przekroczono limit planu dla {country}")
            else:
                logger.error(f"B≈ÇƒÖd wyszukiwania Web Search: {e}")
            return None
    
    def _collect_and_analyze(self, country: str, symbol: str) -> bool:
        """
        Zbiera dane sentymentu u≈ºywajƒÖc:
        1. Web Search (DuckDuckGo/Google/Serper) - g≈Ç√≥wne ≈∫r√≥d≈Ço - LLM sam pobiera aktualne dane z internetu
        2. Twitter/Reddit (fallback) - je≈õli Web Search nie zwr√≥ci wynik√≥w
        
        GDELT ma sw√≥j osobny daemon (gdelt_sentiment_daemon.py) i nie jest u≈ºywany tutaj.
        
        Args:
            country: Kod kraju
            symbol: Symbol kryptowaluty
            
        Returns:
            True je≈õli sukces, False w przeciwnym razie
        """
        try:
            # U≈ºyj query_manager.get_language() je≈õli dostƒôpny (ma pe≈Çniejsze mapowanie)
            # W przeciwnym razie u≈ºyj COUNTRY_LANGUAGES jako fallback
            if self.query_manager:
                language = self.query_manager.get_language(country)
            else:
                language = COUNTRY_LANGUAGES.get(country, "en")
            country_name = COUNTRY_NAMES.get(country, country)
            
            logger.info(f"üìä Zbieram dane sentymentu dla {country} ({language})...")
            
            texts = []
            
            # 1. G≈Ç√≥wne ≈∫r√≥d≈Ço: Web Search (DuckDuckGo/Google/Serper) - LLM sam pobiera dane z internetu
            tavily_query = None
            tavily_response = None
            tavily_answer = None
            tavily_results_count = 0
            
            # DuckDuckGo nie wymaga API key, wiƒôc sprawdzamy tylko czy web_search istnieje
            if self.web_search:
                try:
                    # U≈ºyj spersonalizowanych zapyta≈Ñ regionalnych je≈õli dostƒôpne
                    if self.query_rotator and self.query_manager:
                        # U≈ºyj query_manager.get_language() - on ma pe≈Çne mapowanie REGION_TO_LANGUAGE
                        # NIE u≈ºywaj COUNTRY_LANGUAGES z daemona, bo mo≈ºe byƒá nieaktualne
                        lang = self.query_manager.get_language(country)
                        logger.debug(f"   Mapowanie {country} -> {lang} (przez query_manager)")
                        
                        # Pobierz zapytania dla jƒôzyka (manager ≈Çaduje z pliku {lang}.txt)
                        queries = self.query_rotator.get_fresh(lang, count=2)
                        
                        # Je≈õli brak zapyta≈Ñ dla jƒôzyka, spr√≥buj u≈ºyƒá regionu bezpo≈õrednio
                        if not queries:
                            logger.debug(f"   Brak zapyta≈Ñ dla {lang}, pr√≥bujƒô region {country}")
                            queries = self.query_rotator.get_fresh(country, count=2)
                        
                        if queries:
                            logger.info(f"üîç U≈ºywam {len(queries)} spersonalizowanych zapyta≈Ñ dla {country_name} ({language})")
                            all_results = []
                            all_queries_text = []
                            
                            # Pobierz region i jƒôzyk dla DuckDuckGo na podstawie kraju
                            # UWAGA: language dla prompt√≥w (np. 'en', 'sg', 'de') jest ju≈º ustawiony wcze≈õniej
                            # Tutaj pobieramy jƒôzyk dla DuckDuckGo HTML scraping (np. 'en-US', 'de-DE')
                            duckduckgo_region = COUNTRY_TO_DUCKDUCKGO_REGION.get(country)
                            duckduckgo_language = COUNTRY_TO_DUCKDUCKGO_LANGUAGE.get(country)
                            
                            # Wykonaj ka≈ºde zapytanie osobno
                            for query in queries:
                                try:
                                    logger.debug(f"   ‚Üí {query}")
                                    search_results = self.web_search.search(
                                        query=query,
                                        max_results=3,  # Mniej wynik√≥w na zapytanie, ale wiƒôcej zapyta≈Ñ
                                        search_depth="basic",
                                        include_answer=False,
                                        region=duckduckgo_region,  # Region DuckDuckGo (np. 'us-en', 'de-de')
                                        language=duckduckgo_language  # Jƒôzyk dla HTML scraping (np. 'en-US', 'de-DE')
                                    )
                                    
                                    if search_results.get("success") and search_results.get("results"):
                                        all_results.extend(search_results.get("results", []))
                                        all_queries_text.append(query)
                                    
                                    # Rate limiting - czekaj miƒôdzy zapytaniami
                                    time.sleep(1.0)
                                    
                                except Exception as e:
                                    logger.debug(f"   B≈ÇƒÖd zapytania '{query}': {e}")
                                    continue
                            
                            # Po≈ÇƒÖcz wyniki z wszystkich zapyta≈Ñ
                            if all_results:
                                # Usu≈Ñ duplikaty (po URL)
                                seen_urls = set()
                                unique_results = []
                                for result in all_results:
                                    url = result.get("url", "")
                                    if url and url not in seen_urls:
                                        seen_urls.add(url)
                                        unique_results.append(result)
                                
                                # WyciƒÖgnij teksty z unikalnych wynik√≥w
                                for result in unique_results:
                                    if "title" in result:
                                        texts.append(result["title"])
                                    if "content" in result:
                                        texts.append(result["content"])
                                    elif "snippet" in result:
                                        texts.append(result["snippet"])
                                
                                logger.info(f"üåê Web Search: {len(texts)} tekst√≥w z {len(unique_results)} unikalnych wynik√≥w ({len(all_queries_text)} zapyta≈Ñ)")
                                
                                # Zapisz zapytania i odpowiedzi
                                web_search_query = " | ".join(all_queries_text)  # Wszystkie zapytania oddzielone |
                                # Zapisz pe≈ÇnƒÖ odpowied≈∫ z wynikami (nie tylko metadane)
                                web_search_response = json.dumps({
                                    "queries": all_queries_text,
                                    "results_count": len(unique_results),
                                    "total_results": len(all_results),
                                    "results": unique_results  # Dodaj pe≈Çne wyniki
                                }, ensure_ascii=False)
                                web_search_answer = None  # DuckDuckGo nie zwraca "answer" jak Tavily
                                web_search_results_count = len(unique_results)
                            else:
                                logger.warning(f"‚ö†Ô∏è  Web Search: Brak wynik√≥w z {len(queries)} zapyta≈Ñ dla {country}")
                                web_search_query = " | ".join(queries)
                                web_search_response = json.dumps({"queries": queries, "results": []}, ensure_ascii=False)
                                web_search_answer = None
                                web_search_results_count = 0
                        else:
                            # Fallback do generycznego zapytania
                            logger.debug(f"Brak spersonalizowanych zapyta≈Ñ dla {country}, u≈ºywam generycznego")
                            search_query = f"{symbol} cryptocurrency news {country_name}"
                            search_results = self._execute_web_search(search_query, country)
                            if search_results:
                                texts, web_search_query, web_search_response, web_search_answer, web_search_results_count = search_results
                    else:
                        # Fallback: u≈ºyj generycznego zapytania je≈õli QueryManager niedostƒôpny
                        logger.debug(f"QueryManager niedostƒôpny, u≈ºywam generycznego zapytania dla {country}")
                        search_query = f"{symbol} cryptocurrency news {country_name}"
                        search_results = self._execute_web_search(search_query, country)
                        if search_results:
                            texts, web_search_query, web_search_response, web_search_answer, web_search_results_count = search_results
                    
                except Exception as e:
                    error_str = str(e)
                    if "432" in error_str or "usage limit" in error_str.lower():
                        logger.warning(f"‚ö†Ô∏è  Web Search: Przekroczono limit planu. U≈ºywam fallback do Twitter/Reddit dla {country}")
                    else:
                        logger.error(f"B≈ÇƒÖd wyszukiwania w internecie: {e}")
                        logger.debug(traceback.format_exc())
                    web_search_query = None
                    web_search_response = None
                    web_search_answer = None
                    web_search_results_count = 0
            
            # 2. Fallback: u≈ºyj Twitter/Reddit je≈õli Tavily nie zwr√≥ci≈Ç wystarczajƒÖcej ilo≈õci danych
            # (lub je≈õli Tavily zwr√≥ci≈Ç b≈ÇƒÖd limitu)
            if len(texts) < 5:
                logger.info(f"üì± U≈ºywam Twitter/Reddit jako fallback dla {country} (Web Search: {len(texts)} tekst√≥w)...")
                
                # Reddit
                if self.reddit_collector:
                    try:
                        reddit_posts = self.reddit_collector.get_subreddit_posts(
                            subreddit="cryptocurrency",
                            limit=20  # Zwiƒôkszono limit
                        )
                        if reddit_posts and len(reddit_posts) > 0:
                            # Reddit zwraca listƒô dict, nie DataFrame
                            reddit_texts = [post.get('title', '') for post in reddit_posts if post.get('title')]
                            if reddit_posts[0].get('selftext'):
                                reddit_texts.extend([post.get('selftext', '') for post in reddit_posts if post.get('selftext')])
                            texts.extend(reddit_texts)
                            logger.info(f"üì± Reddit (fallback): {len(reddit_texts)} tekst√≥w z {len(reddit_posts)} post√≥w")
                        else:
                            logger.debug(f"Reddit: brak danych (posts={len(reddit_posts) if reddit_posts else 0})")
                    except Exception as e:
                        logger.warning(f"Reddit niedostƒôpny: {e}")
                
                # Twitter
                if self.twitter_collector and len(texts) < 10:
                    try:
                        tweets = self.twitter_collector.search_tweets(
                            query=f"{symbol} OR cryptocurrency",
                            max_results=20  # Zwiƒôkszono limit
                        )
                        if tweets and len(tweets) > 0:
                            # Twitter zwraca listƒô dict, nie DataFrame
                            twitter_texts = [tweet.get('text', '') for tweet in tweets if tweet.get('text')]
                            texts.extend(twitter_texts)
                            logger.info(f"üê¶ Twitter (fallback): {len(twitter_texts)} tekst√≥w z {len(tweets)} tweet√≥w")
                        else:
                            logger.debug(f"Twitter: brak danych (tweets={len(tweets) if tweets else 0})")
                    except Exception as e:
                        logger.warning(f"Twitter niedostƒôpny: {e}")
                
            
            if not texts:
                logger.warning(f"‚ö†Ô∏è  Brak tekst√≥w do analizy dla {country} (ze wszystkich ≈∫r√≥de≈Ç)")
                return False
            
            logger.info(f"üìù Znaleziono {len(texts)} tekst√≥w do analizy dla {country}")
            
            # Analizuj u≈ºywajƒÖc LLM
            logger.info(f"ü§ñ Analizujƒô sentyment u≈ºywajƒÖc LLM ({self.llm_model})...")
            result = self.llm_analyzer.analyze_sentiment(
                texts=texts,
                region=country,
                language=language,
                symbol=symbol
            )
            
            # Dodaj dane Web Search do wyniku (je≈õli by≈Çy u≈ºyte)
            if web_search_query is not None:
                result["web_search_query"] = web_search_query
                result["web_search_response"] = web_search_response
                result["web_search_answer"] = web_search_answer
                result["web_search_results_count"] = web_search_results_count
            
            # Zaktualizuj rekord w bazie z danymi Web Search (je≈õli by≈Çy zapisane)
            if self.llm_analyzer.save_to_db and symbol and web_search_query is not None:
                try:
                    self._update_web_search_data_in_db(
                        symbol=symbol,
                        region=country,
                        timestamp=result["timestamp"],
                        web_search_query=web_search_query,
                        web_search_response=web_search_response,
                        web_search_answer=web_search_answer,
                        web_search_results_count=web_search_results_count
                    )
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  B≈ÇƒÖd aktualizacji danych Web Search w bazie: {e}")
            
            # Aktualizuj statystyki
            self.stats["analyses_count"] += 1
            self.stats["total_cost_pln"] += result["cost_pln"]
            
            logger.success(
                f"‚úÖ Analiza zako≈Ñczona: {country} - {result['sentiment']} "
                f"(score: {result['score']:+.2f}, cost: {result['cost_pln']:.4f} PLN)"
            )
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd podczas analizy {country}: {e}")
            logger.debug(traceback.format_exc())
            self.stats["errors_count"] += 1
            return False
    
    def _update_cycle(self):
        """Wykonuje jeden cykl aktualizacji."""
        logger.info(f"\n{'='*70}")
        logger.info(f"üîÑ CYKL AKTUALIZACJI #{self.stats['cycles_count'] + 1}")
        logger.info(f"{'='*70}\n")
        
        start_time = time.time()
        success_count = 0
        
        # Dla ka≈ºdego symbolu i kraju
        for symbol in self.symbols:
            for country in self.countries:
                if not self.running:
                    break
                
                if self._collect_and_analyze(country, symbol):
                    success_count += 1
                
                # Ma≈Çe op√≥≈∫nienie miƒôdzy krajami, ≈ºeby nie przeciƒÖ≈ºaƒá API
                time.sleep(2)
        
        elapsed_time = time.time() - start_time
        self.stats["cycles_count"] += 1
        self.stats["last_update"] = datetime.now(timezone.utc)
        
        logger.info(f"\n{'='*70}")
        logger.success(
            f"‚úÖ Cykl zako≈Ñczony: {success_count}/{len(self.symbols) * len(self.countries)} analiz, "
            f"czas: {elapsed_time:.1f}s"
        )
        logger.info(f"üìä Statystyki: {self.stats['analyses_count']} analiz, "
                   f"koszt ≈ÇƒÖczny: {self.stats['total_cost_pln']:.2f} PLN, "
                   f"b≈Çƒôdy: {self.stats['errors_count']}")
        
        # Raport synchronizacji - sprawd≈∫ ile danych jest w bazie
        self._report_data_status()
        
        logger.info(f"{'='*70}\n")
    
    def _report_data_status(self):
        """Raportuje status danych w bazie dla synchronizacji ze strategiƒÖ."""
        try:
            from datetime import timedelta
            
            # Sprawd≈∫ ile unikalnych punkt√≥w czasowych mamy w ostatnich 24h
            end_date = datetime.now(timezone.utc)
            start_date = end_date - timedelta(hours=24)
            
            # Pobierz dane z bazy
            df = self.db.get_llm_sentiment_analysis(
                symbol=self.symbols[0] if self.symbols else "BTC/USDC",
                start_date=start_date,
                end_date=end_date
            )
            
            if df.empty:
                data_points = 0
                regions_count = 0
            else:
                # Policz unikalne punkty czasowe (godzinowe)
                if hasattr(df.index, 'floor'):
                    hourly_points = df.index.floor('H').nunique()
                else:
                    hourly_points = len(df)
                data_points = hourly_points
                regions_count = df['region'].nunique() if 'region' in df.columns else 0
            
            min_required = 24  # Strategia wymaga minimum 24 punkt√≥w
            percentage = min(100, (data_points / min_required) * 100)
            
            if data_points >= min_required:
                logger.success(f"üîÑ SYNCHRONIZACJA: {data_points}/{min_required} punkt√≥w ({percentage:.0f}%) - GOTOWE do pe≈Çnej analizy!")
            else:
                hours_remaining = max(0, min_required - data_points)
                logger.warning(f"üîÑ SYNCHRONIZACJA: {data_points}/{min_required} punkt√≥w ({percentage:.0f}%) - jeszcze ~{hours_remaining}h do pe≈Çnej analizy")
            
            logger.info(f"   Regiony z danymi: {regions_count}/{len(self.countries)}")
            
        except Exception as e:
            logger.debug(f"Nie mo≈ºna sprawdziƒá statusu danych: {e}")
    
    def run(self):
        """Uruchamia daemon."""
        logger.info("üöÄ Uruchamiam LLM Sentiment Daemon...")
        logger.info(f"   Symbole: {', '.join(self.symbols)}")
        logger.info(f"   Kraje: {', '.join(self.countries)}")
        logger.info(f"   Query: {self.query}")
        logger.info(f"   Model LLM: {self.llm_model}")
        logger.info(f"   Interwa≈Ç: {self.update_interval}s ({self.update_interval/60:.1f} min)")
        logger.info(f"   Baza: {self.db._safe_url()}")
        
        # Informacja o synchronizacji z strategiƒÖ
        logger.info("")
        logger.info("üìä SYNCHRONIZACJA ZE STRATEGIƒÑ:")
        logger.info("   ‚Ä¢ Strategia wymaga minimum 24 punkt√≥w czasowych (24h przy resolution 1h)")
        logger.info(f"   ‚Ä¢ Przy interwale {self.update_interval/60:.0f} min i {len(self.countries)} krajach:")
        hours_to_24_points = 24  # Resolution 1h, potrzeba 24 punkt√≥w
        logger.info(f"   ‚Ä¢ Potrzeba ~{hours_to_24_points}h zbierania danych do pe≈Çnej analizy")
        logger.info(f"   ‚Ä¢ Strategia bƒôdzie dzia≈Çaƒá z ograniczonƒÖ dok≈Çadno≈õciƒÖ do tego czasu")
        logger.info("")
        
        self.running = True
        
        try:
            while self.running:
                self._update_cycle()
                
                if not self.running:
                    break
                
                # Poczekaj na nastƒôpny cykl
                logger.info(f"‚è≥ Czekam {self.update_interval}s do nastƒôpnego cyklu...")
                for _ in range(self.update_interval):
                    if not self.running:
                        break
                    time.sleep(1)
        
        except KeyboardInterrupt:
            logger.warning("Przerwano przez u≈ºytkownika")
        except Exception as e:
            logger.error(f"B≈ÇƒÖd w g≈Ç√≥wnej pƒôtli: {e}")
            logger.debug(traceback.format_exc())
        finally:
            logger.info("\n" + "="*70)
            logger.info("üõë Zatrzymywanie daemona...")
            logger.info(f"üìä Ko≈Ñcowe statystyki:")
            logger.info(f"   Cykle: {self.stats['cycles_count']}")
            logger.info(f"   Analizy: {self.stats['analyses_count']}")
            logger.info(f"   B≈Çƒôdy: {self.stats['errors_count']}")
            logger.info(f"   Koszt ≈ÇƒÖczny: {self.stats['total_cost_pln']:.2f} PLN")
            logger.info("="*70)


def main():
    """G≈Ç√≥wna funkcja."""
    parser = argparse.ArgumentParser(
        description="LLM Sentiment Collector Daemon",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Przyk≈Çady:
  # Uruchom z domy≈õlnymi ustawieniami (co 10 min)
  python scripts/llm_sentiment_daemon.py
  
  # Uruchom z niestandardowym interwa≈Çem (5 min)
  python scripts/llm_sentiment_daemon.py --interval=300
  
  # Uruchom dla konkretnych kraj√≥w
  python scripts/llm_sentiment_daemon.py --countries=US,CN,JP
        """
    )
    
    parser.add_argument(
        '--symbols',
        type=str,
        default='BTC/USDC',
        help='Symbole do analizy (oddzielone przecinkami, domy≈õlnie: BTC/USDC)'
    )
    
    parser.add_argument(
        '--countries',
        type=str,
        default='US,CN,JP,KR,DE,GB,RU,SG',
        help='Kraje do analizy (oddzielone przecinkami)'
    )
    
    parser.add_argument(
        '--query',
        type=str,
        default='bitcoin OR BTC OR cryptocurrency',
        help='Zapytanie do GDELT (domy≈õlnie: bitcoin OR BTC OR cryptocurrency)'
    )
    
    parser.add_argument(
        '--interval',
        type=int,
        default=600,
        help='Interwa≈Ç aktualizacji w sekundach (domy≈õlnie: 600 = 10 min)'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='claude-3-5-haiku-20241022',
        help='Model LLM do u≈ºycia (domy≈õlnie: claude-3-5-haiku-20241022 - ta≈Ñszy)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Szczeg√≥≈Çowe logowanie'
    )
    
    args = parser.parse_args()
    
    # Konfiguruj logowanie
    logger.remove()
    level = "DEBUG" if args.verbose else "INFO"
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}",
        level=level,
        colorize=True
    )
    
    # Parsuj argumenty
    symbols = [s.strip() for s in args.symbols.split(",")]
    countries = [c.strip() for c in args.countries.split(",")]
    
    # Utw√≥rz i uruchom daemon
    daemon = LLMSentimentDaemon(
        symbols=symbols,
        countries=countries,
        query=args.query,
        update_interval=args.interval,
        llm_model=args.model
    )
    
    daemon.run()


if __name__ == "__main__":
    main()

