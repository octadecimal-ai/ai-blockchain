"""
API Logger
==========
Modu do logowania wszystkich request贸w i odpowiedzi z API LLM.
Logi s zapisywane TYLKO do pliku, bez wywietlania w konsoli.
"""

import os
import json
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, List
from loguru import logger


class APILogger:
    """
    Logger dla request贸w i odpowiedzi API LLM.
    
    Zapisuje wszystkie requesty i odpowiedzi do osobnego pliku logu.
    Logi NIE s wywietlane w konsoli - tylko zapisywane do pliku.
    """
    
    def __init__(self, log_dir: str = "logs"):
        """
        Inicjalizacja loggera.
        
        Args:
            log_dir: Katalog do zapisu log贸w
        """
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # Utw贸rz cakowicie nowy logger tylko do pliku (bez wywietlania w konsoli)
        # U偶ywamy unikalnej nazwy, aby odr贸偶ni nasze logi od innych
        self.logger_name = "api_llm_file_only"
        
        # Usu domylny handler konsoli (jeli istnieje)
        # Nastpnie dodamy go z powrotem z filtrem, kt贸ry odrzuca nasze logi
        logger.remove()  # Usu wszystkie domylne handlery
        
        # Dodaj handler konsoli z filtrem, kt贸ry odrzuca logi z nasz nazw
        def console_filter(record):
            """Filtr odrzucajcy logi z api_llm_file_only"""
            extra = record.get("extra", {})
            # Odrzu logi z nasz nazw - nie wywietlaj ich w konsoli
            return extra.get("name") != self.logger_name
        
        logger.add(
            sys.stderr,  # Domylny handler konsoli
            level="INFO",
            format="<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <level>{message}</level>",
            filter=console_filter,
            colorize=True
        )
        
        # Dodaj handler do pliku z filtrem
        # Ten handler bdzie akceptowa tylko logi z nasz nazw
        # i nie bdzie propagowa ich do innych handler贸w (konsola)
        log_file = self.log_dir / f"api_llm_requests_{datetime.now().strftime('%Y-%m-%d')}.log"
        
        # Funkcja filtrujca - akceptuje tylko logi z nasz nazw
        def api_log_filter(record):
            """Filtr akceptujcy tylko logi z api_llm_file_only"""
            # Sprawd藕 czy record ma nasz nazw w extra
            extra = record.get("extra", {})
            # Tylko logi z nasz nazw przejd przez filtr
            # Wszystkie inne logi zostan odrzucone przez ten handler
            return extra.get("name") == self.logger_name
        
        handler_id = logger.add(
            str(log_file),
            rotation="1 day",
            retention="90 days",  # Przechowuj logi przez 90 dni
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {message}",
            encoding="utf-8",
            filter=api_log_filter,
            enqueue=False,  # Nie u偶ywaj kolejki, aby unikn propagacji
            colorize=False  # Wycz kolory (niepotrzebne w pliku)
        )
        
        # Zapisz ID handlera
        self._handler_id = handler_id
        
        # Utw贸rz bindowany logger z nasz nazw
        self.api_logger = logger.bind(name=self.logger_name)
        
        # Statystyki token贸w i koszt贸w w sesji
        self.session_stats = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_requests": 0,
            "total_errors": 0,
            "model_usage": {}  # {model: {"input": int, "output": int}}
        }
        
        # Cenniki modeli (USD za 1M token贸w) - input/output
        self.model_pricing = {
            # Anthropic
            "claude-3-5-haiku-20241022": {"input": 0.25, "output": 1.25},
            "claude-3-5-sonnet-20241022": {"input": 3.0, "output": 15.0},
            "claude-3-opus-20240229": {"input": 10.0, "output": 30.0},
            "claude-3-sonnet-20240229": {"input": 3.0, "output": 15.0},
            "claude-3-haiku-20240307": {"input": 0.25, "output": 1.25},
            # OpenAI
            "gpt-4-turbo-preview": {"input": 10.0, "output": 30.0},
            "gpt-4": {"input": 30.0, "output": 60.0},
            "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
        }
        
        # Kurs USD/PLN (mo偶na p贸藕niej pobiera z API)
        self.usd_to_pln = 4.0
    
    def log_request(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, Any]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Loguje request do API LLM.
        
        Args:
            provider: Provider (anthropic, openai)
            model: Nazwa modelu
            messages: Lista wiadomoci (system, user, etc.)
            temperature: Temperatura (opcjonalnie)
            max_tokens: Maksymalna liczba token贸w (opcjonalnie)
            metadata: Dodatkowe metadane (symbol, strategy, etc.)
        """
        request_data = {
            "type": "REQUEST",
            "timestamp": datetime.now().isoformat(),
            "provider": provider,
            "model": model,
            "messages": messages,
            "parameters": {
                "temperature": temperature,
                "max_tokens": max_tokens
            },
            "metadata": metadata or {}
        }
        
        # Formatuj jako JSON dla czytelnoci
        log_message = json.dumps(request_data, ensure_ascii=False, indent=2)
        
        # Loguj tylko do pliku (bez wywietlania w konsoli)
        # U偶ywamy opt(depth=2, colors=False) aby cakowicie pomin propagacj do konsoli
        # Dodatkowo u偶ywamy opt() z parametrem depth=2 aby cakowicie pomin propagacj
        # U偶ywamy opt() z parametrem depth=2 aby cakowicie pomin propagacj do konsoli
        self.api_logger.opt(depth=2, colors=False).info(f"=== API REQUEST ===\n{log_message}")
    
    def log_response(
        self,
        provider: str,
        model: str,
        response_text: str,
        input_tokens: Optional[int] = None,
        output_tokens: Optional[int] = None,
        response_time_ms: Optional[float] = None,
        metadata: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None
    ):
        """
        Loguje odpowied藕 z API LLM.
        
        Args:
            provider: Provider (anthropic, openai)
            model: Nazwa modelu
            response_text: Tekst odpowiedzi
            input_tokens: Liczba token贸w wejciowych (opcjonalnie)
            output_tokens: Liczba token贸w wyjciowych (opcjonalnie)
            response_time_ms: Czas odpowiedzi w ms (opcjonalnie)
            metadata: Dodatkowe metadane (symbol, strategy, etc.)
            error: Bd (jeli wystpi)
        """
        response_data = {
            "type": "RESPONSE",
            "timestamp": datetime.now().isoformat(),
            "provider": provider,
            "model": model,
            "response_text": response_text,
            "usage": {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": (input_tokens or 0) + (output_tokens or 0) if input_tokens or output_tokens else None
            },
            "performance": {
                "response_time_ms": response_time_ms
            },
            "metadata": metadata or {},
            "error": error
        }
        
        # Formatuj jako JSON dla czytelnoci
        log_message = json.dumps(response_data, ensure_ascii=False, indent=2)
        
        # Loguj tylko do pliku (bez wywietlania w konsoli)
        # U偶ywamy opt(depth=2) aby cakowicie pomin propagacj do konsoli
        if error:
            self.api_logger.opt(depth=2, colors=False).error(f"=== API RESPONSE (ERROR) ===\n{log_message}")
            self.session_stats["total_errors"] += 1
        else:
            self.api_logger.opt(depth=2, colors=False).info(f"=== API RESPONSE ===\n{log_message}")
            
            # Aktualizuj statystyki sesji (tylko jeli s tokeny)
            if input_tokens or output_tokens:
                self.session_stats["total_input_tokens"] += (input_tokens or 0)
                self.session_stats["total_output_tokens"] += (output_tokens or 0)
                self.session_stats["total_requests"] += 1
                
                # Aktualizuj statystyki per model
                if model not in self.session_stats["model_usage"]:
                    self.session_stats["model_usage"][model] = {"input": 0, "output": 0}
                self.session_stats["model_usage"][model]["input"] += (input_tokens or 0)
                self.session_stats["model_usage"][model]["output"] += (output_tokens or 0)
    
    def log_request_response_pair(
        self,
        provider: str,
        model: str,
        messages: List[Dict[str, Any]],
        response_text: str,
        input_tokens: Optional[int] = None,
        output_tokens: Optional[int] = None,
        response_time_ms: Optional[float] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None
    ):
        """
        Loguje par request-response w jednym wywoaniu.
        
        Args:
            provider: Provider (anthropic, openai)
            model: Nazwa modelu
            messages: Lista wiadomoci (system, user, etc.)
            response_text: Tekst odpowiedzi
            input_tokens: Liczba token贸w wejciowych (opcjonalnie)
            output_tokens: Liczba token贸w wyjciowych (opcjonalnie)
            response_time_ms: Czas odpowiedzi w ms (opcjonalnie)
            temperature: Temperatura (opcjonalnie)
            max_tokens: Maksymalna liczba token贸w (opcjonalnie)
            metadata: Dodatkowe metadane (symbol, strategy, etc.)
            error: Bd (jeli wystpi)
        """
        self.log_request(
            provider=provider,
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            metadata=metadata
        )
        
        self.log_response(
            provider=provider,
            model=model,
            response_text=response_text,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            response_time_ms=response_time_ms,
            metadata=metadata,
            error=error
        )
    
    def get_session_stats(self) -> Dict[str, Any]:
        """
        Zwraca statystyki sesji (tokeny i koszty).
        
        Returns:
            Sownik ze statystykami
        """
        total_tokens = self.session_stats["total_input_tokens"] + self.session_stats["total_output_tokens"]
        
        # Oblicz koszt w USD
        total_cost_usd = 0.0
        cost_by_model = {}
        
        for model, usage in self.session_stats["model_usage"].items():
            pricing = self.model_pricing.get(model, {"input": 0.0, "output": 0.0})
            
            # Koszt w USD (cena za 1M token贸w)
            input_cost = (usage["input"] / 1_000_000) * pricing["input"]
            output_cost = (usage["output"] / 1_000_000) * pricing["output"]
            model_cost = input_cost + output_cost
            
            total_cost_usd += model_cost
            cost_by_model[model] = {
                "input_tokens": usage["input"],
                "output_tokens": usage["output"],
                "cost_usd": model_cost,
                "cost_pln": model_cost * self.usd_to_pln
            }
        
        # Koszt w PLN
        total_cost_pln = total_cost_usd * self.usd_to_pln
        
        return {
            "total_input_tokens": self.session_stats["total_input_tokens"],
            "total_output_tokens": self.session_stats["total_output_tokens"],
            "total_tokens": total_tokens,
            "total_requests": self.session_stats["total_requests"],
            "total_errors": self.session_stats["total_errors"],
            "total_cost_usd": total_cost_usd,
            "total_cost_pln": total_cost_pln,
            "cost_by_model": cost_by_model,
            "usd_to_pln_rate": self.usd_to_pln
        }
    
    def print_session_stats(self):
        """
        Wywietla statystyki sesji w konsoli (kr贸tka wersja).
        """
        stats = self.get_session_stats()
        
        if stats["total_requests"] == 0:
            return
        
        # Formatuj liczby token贸w
        total_tokens = stats["total_tokens"]
        input_tokens = stats["total_input_tokens"]
        output_tokens = stats["total_output_tokens"]
        
        # Formatuj koszty
        cost_pln = stats["total_cost_pln"]
        
        # Wywietl kr贸tkie statystyki
        logger.info(
            f" API LLM: {total_tokens:,} token贸w "
            f"({input_tokens:,} in + {output_tokens:,} out) | "
            f"Koszt: {cost_pln:.4f} PLN"
        )


# Singleton instance
_api_logger_instance: Optional[APILogger] = None


def get_api_logger() -> APILogger:
    """
    Zwraca singleton instance APILogger.
    
    Returns:
        APILogger instance
    """
    global _api_logger_instance
    if _api_logger_instance is None:
        _api_logger_instance = APILogger()
    return _api_logger_instance
